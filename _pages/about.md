---
permalink: /
title: "Academic Pages is a ready-to-fork GitHub Pages template for academic personal websites"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

This is the front page of a website that is powered by the [Academic Pages template](https://github.com/academicpages/academicpages.github.io) and hosted on GitHub pages. [GitHub pages](https://pages.github.com) is a free service in which websites are built and hosted from code and data stored in a GitHub repository, automatically updating when a new commit is made to the repository.

For the past two years, I have worked with Professor Li Shen, and recently with Professors Lingyao Li and Mark Yatskar. Under these advisors, I have been fortunate to conduct research that has shaped my interests into two distinct but reinforcing directions: (1) developing a “physics” of large language models (LLMs) i.e. a principled understanding of LLMs’ knowledge and reasoning, and (2) studying these dynamics in biomedicine, where domain expertise, robust generalization and behavioral control are crucial.

**Physics of LLMs.** Beyond scaling laws for parameters, data, and compute, we largely lack a scientific account of LLMs’ capabilities, hampering our ability to principally control them or improve their generalization.

I first encountered this gap while teaching LLMs out-of-distribution hospital protocols. In our analysis, presented at AAAI 2025 GenAI4Health Workshop (Lee et al., 2025), beyond biases that I counterfactually identified, models also struggled to learn the knowledge from continued pre-training, a failure mode reflected in prior works (Ovadia et al., 2023; Jiang et al., 2024). This inability to reproduce pre-training for continual learning motivated my subsequent research, in review for ICLR 2026, where I hypothesize that natural language representation of knowledge matters. We essentially asked: “Can an LLM learn calculus by simply reading lecture notes, or does it require a teacher to rephrase concepts, work through problems, and spell everything out?” Through a carefully controlled experiment using fine-grained probes (building on Chang et al., 2024), I found that auxiliary views, pedagogical formulations that present knowledge in varied contexts, are crucial for models to internalize new information. This suggests a fundamental limitation: while pre-training largely bypasses this by massively sampling from the human writing distribution, LLMs will struggle with long-tail information where they lack auxiliary coverage (e.g., blogs, StackExchange discussions) unless they possess the capability to autonomously produce true explanations for out-of-distribution text.

These experiences have convinced me that LLMs need more physics-like investigations: tightly controlled experiments that isolate causal factors. For example, in follow-up experiments, I paradoxically observed that disrupting local discourse by shuffling sentences and paragraphs improved knowledge acquisition. While this warrants investigation, this leads me to hypothesize that breaking sequential discourse in documents enforces a harder objective of reconstructing knowledge without those markers. I also identified the importance of filling in prerequisite knowledge for domain adaptation. I plan to examine this "connectedness" of knowledge: how close must a new piece of information be to the pre-trained knowledge topology to be learnable? Beyond knowledge acquisition, I’m interested in studying the entanglement of pretraining and reasoning (Yue et al. 2025).

To support these controlled experiments, I plan to develop better evaluation probes and datasets. Preventing leakage between synthetic data and probes was a major challenge in the previous work, demanding novel overlap metrics and detection pipelines. Thus, I am interested in leveraging domains with distinct entities rooted in a shared topology, such as chemical molecules, for future studies. Furthermore, the field often relies on probes that measure the log-probability of a correct token. However, complex understanding surfaces saliently in long-form generation, not rote completion. Thus, I intend to develop pipelines that inject knowledge, apply minimally needed instruct-tuning, and test for complex synthesis (e.g., closed-book summarization). Lastly, to better evaluate out-of-distribution generalization of current reasoning patterns in LLMs, I plan to collect and design small, controlled environments, like chess puzzles or case studies, that are outside of common RL recipes.

**Biomedical applications.** I also aim to tackle challenges specific to using LLMs in biomedicine, a domain where LLMs are uniquely positioned to provide significant impact but also demands robust generalization (e.g. to new drugs or unseen mutations) and safety (e.g. being overly confident in uncertain diagnoses).

Leveraging biomedical corpora with Dr. Li Shen, I saw the scale of scientific literature and how much of it remained effectively inaccessible to researchers. This shaped my conviction that LLMs are uniquely positioned to integrate and synthesize this vast information, particularly in contrast to a protein binding project where I found deep learning models fundamentally constrained by labeled data availability. In a later project, presented at AMIA Informatics Summit 2025 and recognised with a Distinguished Paper award (Lee et al. 2025), I novelly identified that LLMs already encode surprisingly detailed knowledge of genetic variants such as SNPs. To address high dimensionality and multiple testing in genetics, I built a system leveraging LLMs as knowledge priors to perform feature selection and engineering for downstream tasks.

This view of LLMs as “knowledge houses” informs my ongoing work with Dr. Mark Yatskar on instilling molecular chemistry into LLMs. We face a distinct "modality gap": molecular representations (e.g., SMILES) are not used in scientific writing, and not all molecules have names. To address this, we use models with a separate representation for molecules and I am currently designing mid-training recipes to bridge these two modalities together. Here, I plan to investigate how well linguistic knowledge generalizes to true chemical topology. By replacing entity mentions with molecule tokens, I aim to induce a fused representation space and divide entities to analyze how natural language knowledge of seen molecules generalizes to unseen ones.

While these works have shown me the potential of LLMs as tools for scientific discovery, my current work with Dr. Lingyao Li on designing medical agents has exposed me to their failures, such as reasoning patterns that fail to generalize to many clinical tasks. I aim to additionally address failure modes like sycophancy, an artifact of human preferences (Sharma et al., 2023). By establishing a taxonomy of valid reasoning patterns, I plan to generate targeted synthetic data that aligns models toward these desired behaviors.

